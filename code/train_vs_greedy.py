"""
å¯¹æŠ—è®­ç»ƒè„šæœ¬
è®­ç»ƒagentå¯¹æŠ—Greedyç­–ç•¥ï¼Œè¿™æ ·å¯ä»¥å­¦ä¹ å¦‚ä½•å‡»è´¥å¯¹æ‰‹
"""
import datetime
import os
import numpy as np
import argparse
from tqdm import tqdm

from gymnasium.spaces import Box, Discrete, Dict as GymDict
import gymnasium as gym

import ray
from ray.tune.registry import register_env
from ray.rllib.env.wrappers.pettingzoo_env import PettingZooEnv
from ray.rllib.algorithms.ppo import PPO, PPOConfig
from ray.rllib.core.rl_module.rl_module import SingleAgentRLModuleSpec
from ray.rllib.policy.policy import Policy

from ChineseChecker import chinese_checker_v0
from ChineseChecker.models.action_masking_rlm import TorchActionMaskRLM
from ChineseChecker.logger import custom_log_creator
from agents import GreedyPolicy

class SingleAgentVsOpponent(gym.Env):
    """å•agentç¯å¢ƒåŒ…è£…å™¨ï¼šagentå¯¹æŠ—å›ºå®šå¯¹æ‰‹ï¼ˆGreedyæˆ–RL Baselineï¼‰"""
    def __init__(self, triangle_size=2, max_iters=200, opponent_type='greedy', rl_opponent_policy=None):
        super().__init__()
        self.triangle_size = triangle_size
        self.max_iters = max_iters
        self.opponent_type = opponent_type
        
        # åˆå§‹åŒ–å¯¹æ‰‹
        if opponent_type == 'greedy':
            self.opponent = GreedyPolicy(triangle_size)
        elif opponent_type == 'random':
            self.opponent = None  # éšæœºå¯¹æ‰‹
        elif opponent_type == 'rl_baseline':
            if rl_opponent_policy is None:
                self.opponent = Policy.from_checkpoint("pretrained/policies/default_policy")
            else:
                self.opponent = rl_opponent_policy
        else:
            raise ValueError(f"Unknown opponent type: {opponent_type}")
            
        self.env = chinese_checker_v0.env(
            render_mode=None, 
            triangle_size=triangle_size, 
            max_iters=max_iters
        )
        
        # å®šä¹‰è§‚æµ‹å’ŒåŠ¨ä½œç©ºé—´
        action_space_dim = (4 * triangle_size + 1) ** 2 * 6 * 2 + 1
        observation_space_dim = (4 * triangle_size + 1) ** 2 * 4
        
        # ä½¿ç”¨Dictç©ºé—´åŒ…å«observationå’Œaction_mask
        self.observation_space = GymDict({
            "observation": Box(low=0, high=1, shape=(observation_space_dim,), dtype=np.int8),
            "action_mask": Box(low=0, high=1, shape=(action_space_dim,), dtype=np.int8)
        })
        self.action_space = Discrete(action_space_dim)
        
    def reset(self, seed=None, options=None):
        """é‡ç½®ç¯å¢ƒ"""
        self.env.reset(seed=seed)
        
        # å¦‚æœç¬¬ä¸€ä¸ªç©å®¶æ˜¯å¯¹æ‰‹ï¼Œè®©å®ƒå…ˆèµ°
        if self.env.agent_selection == self.env.possible_agents[1]:
            obs, reward, termination, truncation, info = self.env.last()
            if not (termination or truncation):
                action = self.opponent.compute_single_action(obs)[0]
                self.env.step(int(action))
        
        # è¿”å›å­¦ä¹ agentçš„è§‚æµ‹
        obs, reward, termination, truncation, info = self.env.last()
        return obs, info
    
    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        my_agent = self.env.possible_agents[0]  # player_0
        
        # å­¦ä¹ agentèµ°ä¸€æ­¥
        self.env.step(int(action))
        obs, env_reward, terminated, truncated, info = self.env.last()
        
        done = terminated or truncated
        reward = 0  # é»˜è®¤æ²¡æœ‰å¥–åŠ±
        
        # å¦‚æœæ¸¸æˆç»“æŸï¼Œæ£€æŸ¥è°èµ¢äº†
        if done:
            winner = self.env.unwrapped.winner
            if winner == my_agent:
                reward = 100  # èµ¢äº†
            elif winner is None:
                reward = -10  # å¹³å±€/è¶…æ—¶
            else:
                reward = -100  # è¾“äº†
        else:
            # è®©å¯¹æ‰‹èµ°
            if self.env.agent_selection == self.env.possible_agents[1]:
                opp_obs = obs
                # éšæœºå¯¹æ‰‹ï¼šéšæœºé€‰æ‹©åˆæ³•åŠ¨ä½œ
                if self.opponent is None:
                    action_mask = opp_obs["action_mask"]
                    legal_actions = np.where(action_mask == 1)[0]
                    opp_action = np.random.choice(legal_actions) if len(legal_actions) > 0 else 0
                else:
                    opp_action = self.opponent.compute_single_action(opp_obs)[0]
                self.env.step(int(opp_action))
                obs, opp_reward, terminated, truncated, info = self.env.last()
                
                done = terminated or truncated
                if done:
                    winner = self.env.unwrapped.winner
                    if winner == my_agent:
                        reward = 100
                    elif winner is None:
                        reward = -10
                    else:
                        reward = -100
        
        return obs, reward, done, done, info
    
    def _compute_progress_from_obs(self, obs):
        """ä»observationè®¡ç®—æ£‹å­å‘ç›®æ ‡åŒºåŸŸçš„è¿›åº¦ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰"""
        n = self.triangle_size
        board_size = 4 * n + 1
        
        # è§£æobservationè·å–æ£‹å­ä½ç½®
        if isinstance(obs, dict):
            observation = obs["observation"].reshape(board_size, board_size, 4)
        else:
            observation = obs.reshape(board_size, board_size, 4)
        
        # ç›®æ ‡åŒºåŸŸä½ç½®ï¼ˆä¸agents.pyä¸­MinimaxPolicyä¸€è‡´ï¼‰
        target_positions = set()
        for i in range(n):
            for j in range(0, n - i):
                q = -n + j
                r = n + 1 + i
                target_positions.add((q, r))
        
        total_progress = 0
        # é€šé“0æ˜¯å½“å‰ç©å®¶ï¼ˆplayer_0ï¼‰çš„æ£‹å­
        for qi in range(board_size):
            for ri in range(board_size):
                if observation[qi, ri, 0] == 1:  # æˆ‘çš„æ£‹å­
                    q = qi - 2 * n
                    r = ri - 2 * n
                    
                    # è®¡ç®—åˆ°ç›®æ ‡åŒºåŸŸçš„æœ€å°è·ç¦»çš„è´Ÿå€¼ï¼ˆè¶Šè¿‘è¶Šå¤§ï¼‰
                    if target_positions:
                        min_dist = min(abs(q - t[0]) + abs(r - t[1]) for t in target_positions)
                        total_progress -= min_dist
        
        return total_progress


def create_config(env_name: str, triangle_size: int = 4, num_workers: int = 8, use_large_network: bool = False):
    """åˆ›å»ºPPOé…ç½®"""
    rlm_class = TorchActionMaskRLM
    # å¦‚æœè¦ä»pretrainedåŠ è½½ï¼Œå¿…é¡»ç”¨ç›¸åŒçš„ç½‘ç»œç»“æ„ [256, 128]
    if use_large_network:
        model_config = {"fcnet_hiddens": [512, 512, 256]}  # æ›´å¤§çš„ç½‘ç»œ
    else:
        model_config = {"fcnet_hiddens": [256, 128]}  # ä¸pretrainedä¸€è‡´
    rlm_spec = SingleAgentRLModuleSpec(module_class=rlm_class, model_config_dict=model_config)

    action_space_dim = (4 * triangle_size + 1) ** 2 * 6 * 2 + 1
    observation_space_dim = (4 * triangle_size + 1) ** 2 * 4

    # è‡ªåŠ¨æ£€æµ‹GPU
    import torch
    num_gpus = 1 if torch.cuda.is_available() else 0
    if num_gpus > 0:
        print(f"æ£€æµ‹åˆ°GPU: {torch.cuda.get_device_name(0)}, å°†ä½¿ç”¨GPUè®­ç»ƒ")
    else:
        print("æœªæ£€æµ‹åˆ°GPU, ä½¿ç”¨CPUè®­ç»ƒ")
    print(f"ä½¿ç”¨ {num_workers} ä¸ªå¹¶è¡Œworkerè¿›è¡Œç¯å¢ƒé‡‡æ ·")

    config = (
        PPOConfig()
        .environment(
            env=env_name,
            clip_actions=True,
            env_config={
                "triangle_size": triangle_size,
                "max_iters": 200,
            },
        )
        .rollouts(
            num_rollout_workers=num_workers,  # å¢åŠ å¹¶è¡Œworker
            num_envs_per_worker=4,            # æ¯ä¸ªworkerè¿è¡Œ4ä¸ªç¯å¢ƒ
            rollout_fragment_length="auto",
        )
        .training(
            train_batch_size=2048,            # å‡å°batch sizeåŠ å¿«è¿­ä»£
            lr=1e-5,                          # æ›´ä½å­¦ä¹ ç‡ï¼Œé˜²æ­¢ç ´åé¢„è®­ç»ƒæƒé‡
            gamma=0.995,
            lambda_=0.95,
            use_gae=True,
            clip_param=0.1,                   # æ›´å°çš„clipï¼Œæ›´ä¿å®ˆæ›´æ–°
            grad_clip=0.5,
            vf_loss_coeff=0.5,
            sgd_minibatch_size=256,           # å¯¹åº”è°ƒæ•´minibatch
            num_sgd_iter=3,                   # æ›´å°‘SGDè¿­ä»£
            entropy_coeff=0.005,              # é™ä½ç†µï¼Œæ›´ç¡®å®šæ€§
            _enable_learner_api=True
        )
        .experimental(_disable_preprocessor_api=True)
        .framework("torch")
        .resources(num_gpus=num_gpus)
        .rl_module(rl_module_spec=rlm_spec)
    )
    return config


def evaluate_vs_greedy(policy, triangle_size, num_trials=20, verbose=False):
    """è¯„ä¼°ç­–ç•¥å¯¹æŠ—Greedy"""
    env = chinese_checker_v0.env(render_mode=None, triangle_size=triangle_size, max_iters=100)
    greedy = GreedyPolicy(triangle_size)
    
    wins = 0
    for i in range(num_trials):
        env.reset(seed=i)
        step_count = 0
        for agent in env.agent_iter():
            obs, reward, termination, truncation, info = env.last()
            if termination or truncation:
                break
            step_count += 1
            
            if agent == env.possible_agents[0]:
                # RLç­–ç•¥ï¼šobså·²ç»æ˜¯dictæ ¼å¼ï¼Œç›´æ¥ä¼ å…¥
                action = policy.compute_single_action(obs)[0]
            else:
                action = greedy.compute_single_action(obs)[0]
            env.step(int(action))
        
        winner = env.unwrapped.winner
        if verbose and i == 0:
            print(f"  [Debug] Game {i}: steps={step_count}, winner={winner}, possible_agents={env.possible_agents}")
        if winner == env.possible_agents[0]:
            wins += 1
    
    return wins / num_trials


def evaluate_vs_random(policy, triangle_size, num_trials=20):
    """è¯„ä¼°ç­–ç•¥å¯¹æŠ—éšæœºå¯¹æ‰‹"""
    env = chinese_checker_v0.env(render_mode=None, triangle_size=triangle_size, max_iters=100)
    
    wins = 0
    for i in range(num_trials):
        env.reset(seed=i)
        for agent in env.agent_iter():
            obs, reward, termination, truncation, info = env.last()
            if termination or truncation:
                break
            
            if agent == env.possible_agents[0]:
                # RLç­–ç•¥ï¼šobså·²ç»æ˜¯dictæ ¼å¼ï¼Œç›´æ¥ä¼ å…¥
                action = policy.compute_single_action(obs)[0]
            else:
                # éšæœºå¯¹æ‰‹
                action_mask = obs["action_mask"]
                legal_actions = np.where(action_mask == 1)[0]
                action = np.random.choice(legal_actions) if len(legal_actions) > 0 else 0
            env.step(int(action))
        
        if env.unwrapped.winner == env.possible_agents[0]:
            wins += 1
    
    return wins / num_trials


def evaluate_vs_rl_baseline(policy, rl_baseline, triangle_size, num_trials=20):
    """è¯„ä¼°ç­–ç•¥å¯¹æŠ—RL Baseline"""
    env = chinese_checker_v0.env(render_mode=None, triangle_size=triangle_size, max_iters=100)
    
    wins = 0
    for i in range(num_trials):
        env.reset(seed=i)
        for agent in env.agent_iter():
            obs, reward, termination, truncation, info = env.last()
            if termination or truncation:
                break
            
            if agent == env.possible_agents[0]:
                # RLç­–ç•¥ï¼šobså·²ç»æ˜¯dictæ ¼å¼ï¼Œç›´æ¥ä¼ å…¥
                action = policy.compute_single_action(obs)[0]
            else:
                # RL Baseline
                action = rl_baseline.compute_single_action(obs)[0]
            env.step(int(action))
        
        if env.unwrapped.winner == env.possible_agents[0]:
            wins += 1
    
    return wins / num_trials


def train_vs_greedy_env(policy, greedy_policy, env, num_episodes=100):
    """
    è®©RLç­–ç•¥ä¸Greedyå¯¹å¼ˆæ”¶é›†ç»éªŒ
    è¿”å›transitionsç”¨äºè®­ç»ƒ
    """
    transitions = []
    
    for ep in range(num_episodes):
        env.reset(seed=ep)
        episode_data = []
        
        for agent in env.agent_iter():
            obs, reward, termination, truncation, info = env.last()
            if termination or truncation:
                break
            
            if agent == env.possible_agents[0]:
                # RL agent
                action = policy.compute_single_action(obs)[0]
                episode_data.append({
                    'obs': obs,
                    'action': action,
                    'reward': reward
                })
            else:
                # Greedy opponent
                action = greedy_policy.compute_single_action(obs)[0]
            
            env.step(int(action))
        
        # æ·»åŠ æœ€ç»ˆå¥–åŠ±
        if episode_data:
            final_reward = 1000 if env.unwrapped.winner == env.possible_agents[0] else -500
            episode_data[-1]['reward'] += final_reward
        
        transitions.extend(episode_data)
    
    return transitions


def main(args):
    """ä¸»å‡½æ•° - ä¸‰é˜¶æ®µè®­ç»ƒ"""
    
    # é˜¶æ®µ0ç¯å¢ƒï¼šå¯¹æŠ—éšæœºï¼ˆé¢„è®­ç»ƒï¼‰
    def env_creator_random(config):
        return SingleAgentVsOpponent(
            triangle_size=config.get("triangle_size", 2),
            max_iters=config.get("max_iters", 100),
            opponent_type='random'
        )
    
    # é˜¶æ®µ1ç¯å¢ƒï¼šå¯¹æŠ—Greedy
    def env_creator_greedy(config):
        return SingleAgentVsOpponent(
            triangle_size=config.get("triangle_size", 2),
            max_iters=config.get("max_iters", 100),
            opponent_type='greedy'
        )
    
    # é˜¶æ®µ2ç¯å¢ƒï¼šå¯¹æŠ—RL Baseline
    def env_creator_rl(config):
        return SingleAgentVsOpponent(
            triangle_size=config.get("triangle_size", 2),
            max_iters=config.get("max_iters", 100),
            opponent_type='rl_baseline'
        )

    env_name = 'single_vs_opponent'
    
    # æ ¹æ®æ˜¯å¦ä»pretrainedå¼€å§‹ï¼Œé€‰æ‹©åˆå§‹ç¯å¢ƒ
    if args.start_from_pretrained:
        # ä»pretrainedå¼€å§‹ï¼Œç›´æ¥æ³¨å†ŒGreedyç¯å¢ƒï¼ˆé˜¶æ®µ1ï¼‰
        register_env(env_name, env_creator_greedy)
        phase = 1
        phase0_completed = True
    else:
        # ä»å¤´å¼€å§‹ï¼Œæ³¨å†ŒRandomç¯å¢ƒï¼ˆé˜¶æ®µ0ï¼‰
        register_env(env_name, env_creator_random)
        phase = 0
        phase0_completed = False

    ray.init(num_cpus=args.num_cpus or None, local_mode=args.local_mode)
    
    # å¦‚æœä»pretrainedå¼€å§‹ï¼Œç”¨å°ç½‘ç»œï¼›å¦åˆ™ç”¨å¤§ç½‘ç»œ
    use_large = not args.start_from_pretrained
    config = create_config(env_name, args.triangle_size, args.num_workers, use_large_network=use_large)
    
    timestr = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    logdir = f"logs/three_stage_{timestr}"
    os.makedirs(logdir, exist_ok=True)
    
    algo = config.build(logger_creator=custom_log_creator(os.path.join(os.curdir, logdir), ''))
    
    # ä»checkpointæ¢å¤æƒé‡
    if args.restore_from:
        print(f"ä»checkpointæ¢å¤æƒé‡: {args.restore_from}")
        try:
            # å°è¯•åŠ è½½policyçš„æƒé‡
            restored_policy = Policy.from_checkpoint(os.path.join(args.restore_from, "policies", "default_policy"))
            current_policy = algo.get_policy("default_policy")
            current_policy.set_weights(restored_policy.get_weights())
            # åŒæ­¥æƒé‡åˆ°æ‰€æœ‰worker
            weights_to_sync = {"default_policy": restored_policy.get_weights()}
            algo.workers.foreach_worker(lambda w: w.set_weights(weights_to_sync))
            print("æˆåŠŸæ¢å¤æƒé‡å¹¶åŒæ­¥åˆ°æ‰€æœ‰worker!")
        except Exception as e:
            print(f"æ— æ³•ä»checkpointæ¢å¤æƒé‡: {e}")
            print("å°†ä»å¤´å¼€å§‹è®­ç»ƒ...")
    elif args.start_from_pretrained:
        # ä»pretrained RL Baselineå¼€å§‹ï¼Œè·³è¿‡é˜¶æ®µ0
        print("ä»pretrained RL Baselineå¼€å§‹è®­ç»ƒ...")
        try:
            restored_policy = Policy.from_checkpoint("pretrained/policies/default_policy")
            restored_weights = restored_policy.get_weights()
            
            # è®¾ç½®main policyæƒé‡
            current_policy = algo.get_policy("default_policy")
            current_policy.set_weights(restored_weights)
            
            # æ–¹æ³•1: ä½¿ç”¨local_workeråŒæ­¥
            algo.workers.local_worker().set_weights({"default_policy": restored_weights})
            
            # æ–¹æ³•2: é€ä¸ªåŒæ­¥åˆ°remote workers
            def set_weights_fn(worker):
                worker.set_weights({"default_policy": restored_weights})
            algo.workers.foreach_worker(set_weights_fn, local_worker=False)
            
            # â˜… å…³é”®ï¼šåŒæ­¥åˆ°Learneræ¨¡å— â˜…
            # æ–°RLlib APIä¸­ï¼ŒLearneræœ‰ç‹¬ç«‹çš„æƒé‡å‰¯æœ¬
            if hasattr(algo, 'learner_group') and algo.learner_group is not None:
                # è·å–RLModuleçš„state dictæ ¼å¼
                rl_module = current_policy.model
                learner_weights = {"default_policy": rl_module.state_dict()}
                algo.learner_group.set_weights(learner_weights)
                print("å·²åŒæ­¥æƒé‡åˆ°Learneræ¨¡å—!")
            
            print("æˆåŠŸä»pretrainedåŠ è½½æƒé‡!")
            
            # éªŒè¯local workerçš„policy
            print("éªŒè¯æƒé‡åŒæ­¥...")
            local_policy = algo.workers.local_worker().get_policy("default_policy")
            test_winrate = evaluate_vs_random(local_policy, args.triangle_size, num_trials=10)
            print(f"  local worker policy vs Random: {test_winrate*100:.0f}%")
            test_winrate_greedy = evaluate_vs_greedy(local_policy, args.triangle_size, num_trials=10)
            print(f"  local worker policy vs Greedy: {test_winrate_greedy*100:.0f}%")
            
            # éªŒè¯remote worker (æŠ½æŸ¥ä¸€ä¸ª)
            def get_remote_winrate(worker):
                p = worker.get_policy("default_policy")
                # ç®€å•æµ‹è¯•ï¼šè¿”å›æƒé‡çš„ä¸€ä¸ªå€¼æ¥ç¡®è®¤æ˜¯å¦åŒæ­¥
                w = p.get_weights()
                first_key = list(w.keys())[0]
                return float(w[first_key].flat[0])
            
            remote_check = algo.workers.foreach_worker(get_remote_winrate, local_worker=False)
            local_check = get_remote_winrate(algo.workers.local_worker())
            print(f"  æƒé‡ä¸€è‡´æ€§æ£€æŸ¥: local={local_check:.6f}, remote[0]={remote_check[0]:.6f}")
            if abs(local_check - remote_check[0]) < 1e-6:
                print("  âœ“ æƒé‡å·²åŒæ­¥åˆ°æ‰€æœ‰worker!")
            else:
                print("  âœ— è­¦å‘Š: æƒé‡å¯èƒ½æœªæ­£ç¡®åŒæ­¥!")
            
            # è·³è¿‡é˜¶æ®µ0
            phase0_completed = True
            phase = 1  # ç›´æ¥è¿›å…¥é˜¶æ®µ1
        except Exception as e:
            print(f"æ— æ³•ä»pretrainedåŠ è½½æƒé‡: {e}")
            import traceback
            traceback.print_exc()
            print("å°†ä»é˜¶æ®µ0å¼€å§‹è®­ç»ƒ...")
    
    greedy = GreedyPolicy(args.triangle_size)
    
    # åŠ è½½RL Baselineç”¨äºè¯„ä¼°
    rl_baseline = Policy.from_checkpoint("pretrained/policies/default_policy")
    
    best_winrate_random = 0.0
    best_winrate_greedy = 0.0
    best_winrate_rl = 0.0
    # phaseå’Œphase0_completedå·²åœ¨ä¸Šé¢æ ¹æ®args.start_from_pretrainedè®¾ç½®
    phase1_completed = False
    
    if phase == 0:
        print("=" * 60)
        print("é˜¶æ®µ0: å¯¹æŠ—Randomé¢„è®­ç»ƒ (ç›®æ ‡: 90%+)")
        print("=" * 60)
    else:
        print("=" * 60)
        print("é˜¶æ®µ1: å¯¹æŠ—Greedyè®­ç»ƒ (ç›®æ ‡: 90%+) - ä»pretrainedå¼€å§‹")
        print("=" * 60)
    
    # ä¿å­˜è®­ç»ƒå‰çš„æƒé‡ç”¨äºå¯¹æ¯”
    if args.start_from_pretrained:
        pre_train_weights = algo.get_policy("default_policy").get_weights()
        first_key = list(pre_train_weights.keys())[0]
        pre_train_sample = pre_train_weights[first_key].copy()
    
    for i in range(args.train_iters):
        # è®­ç»ƒä¸€æ¬¡è¿­ä»£
        result = algo.train()
        
        # è·å–ç­–ç•¥
        policy = algo.get_policy("default_policy")
        
        # ç¬¬ä¸€æ¬¡è¿­ä»£ï¼šæ£€æŸ¥æƒé‡å˜åŒ–
        if i == 0 and args.start_from_pretrained:
            post_train_weights = policy.get_weights()
            post_train_sample = post_train_weights[first_key]
            weight_diff = np.abs(post_train_sample - pre_train_sample).mean()
            weight_max_diff = np.abs(post_train_sample - pre_train_sample).max()
            print(f"[æƒé‡å˜åŒ–è¯Šæ–­] mean_diff={weight_diff:.6f}, max_diff={weight_max_diff:.6f}")
            print(f"  åŸå§‹æƒé‡èŒƒå›´: [{pre_train_sample.min():.4f}, {pre_train_sample.max():.4f}]")
            print(f"  è®­ç»ƒåæƒé‡èŒƒå›´: [{post_train_sample.min():.4f}, {post_train_sample.max():.4f}]")
        
        # æ¯Næ¬¡è¯„ä¼°ä¸€ä¸‹
        if i % args.eval_period == 0:
            # ç¬¬ä¸€æ¬¡è¯„ä¼°åŠ è°ƒè¯•ä¿¡æ¯
            verbose = (i == 0)
            winrate_random = evaluate_vs_random(policy, args.triangle_size, num_trials=10)
            winrate_greedy = evaluate_vs_greedy(policy, args.triangle_size, num_trials=10, verbose=verbose)
            winrate_rl = evaluate_vs_rl_baseline(policy, rl_baseline, args.triangle_size, num_trials=10)
            
            print(f"[é˜¶æ®µ{phase}] Iter {i}: reward={result['episode_reward_mean']:.1f}, "
                  f"vs_Random={winrate_random*100:.0f}%, vs_Greedy={winrate_greedy*100:.0f}%, vs_RL={winrate_rl*100:.0f}%")
            
            # ä¿å­˜vs Randomæœ€å¥½çš„æ¨¡å‹
            if winrate_random > best_winrate_random:
                best_winrate_random = winrate_random
            
            # ä¿å­˜vs Greedyæœ€å¥½çš„æ¨¡å‹
            if winrate_greedy > best_winrate_greedy:
                best_winrate_greedy = winrate_greedy
                checkpoint_dir = f"{logdir}/best_vs_greedy"
                algo.save(checkpoint_dir=checkpoint_dir)
                print(f"  -> æ–°æœ€ä½³vs Greedy: {winrate_greedy*100:.0f}%")
            
            # ä¿å­˜vs RLæœ€å¥½çš„æ¨¡å‹
            if winrate_rl > best_winrate_rl:
                best_winrate_rl = winrate_rl
                checkpoint_dir = f"{logdir}/best_vs_rl"
                algo.save(checkpoint_dir=checkpoint_dir)
                print(f"  -> æ–°æœ€ä½³vs RL: {winrate_rl*100:.0f}%")
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é˜¶æ®µ0ç›®æ ‡ï¼ˆvs Random 90%+ï¼‰
            if phase == 0 and winrate_random >= 0.90 and not phase0_completed:
                phase0_completed = True
                print("\n" + "=" * 60)
                print(f"ğŸ‰ é˜¶æ®µ0å®Œæˆ! vs Randomè¾¾åˆ° {winrate_random*100:.0f}%")
                print("ç°åœ¨åˆ‡æ¢åˆ°é˜¶æ®µ1: å¯¹æŠ—Greedy (ç›®æ ‡: 90%+)")
                print("=" * 60 + "\n")
                
                # ä¿å­˜checkpointåˆ°æ–‡ä»¶ï¼ˆæ›´å¯é ï¼‰
                phase0_checkpoint = f"{logdir}/phase0_completed"
                algo.save(checkpoint_dir=phase0_checkpoint)
                print(f"å·²ä¿å­˜é˜¶æ®µ0 checkpointåˆ°: {phase0_checkpoint}")
                
                # åœæ­¢å½“å‰ç®—æ³•
                algo.stop()
                
                # é‡æ–°æ³¨å†Œç¯å¢ƒä¸ºGreedy
                register_env(env_name, env_creator_greedy)
                config = create_config(env_name, args.triangle_size, args.num_workers, use_large_network=use_large)
                algo = config.build(logger_creator=custom_log_creator(os.path.join(os.curdir, logdir), ''))
                
                # ä»checkpointæ¢å¤æƒé‡
                policy_path = os.path.join(phase0_checkpoint, "policies", "default_policy")
                print(f"ä»checkpointåŠ è½½æƒé‡: {policy_path}")
                restored_policy = Policy.from_checkpoint(policy_path)
                restored_weights = restored_policy.get_weights()
                
                current_policy = algo.get_policy("default_policy")
                current_policy.set_weights(restored_weights)
                
                # åŒæ­¥åˆ°æ‰€æœ‰worker
                algo.workers.local_worker().set_weights({"default_policy": restored_weights})
                def set_weights_fn(worker):
                    worker.set_weights({"default_policy": restored_weights})
                algo.workers.foreach_worker(set_weights_fn, local_worker=False)
                
                # åŒæ­¥åˆ°Learneræ¨¡å—ï¼ˆå…³é”®ï¼ï¼‰
                learner_group = algo.learner_group
                def update_learner_weights(learner):
                    learner._module["default_policy"].load_state_dict(
                        current_policy.model.state_dict()
                    )
                learner_group.foreach_learner(update_learner_weights)
                
                # éªŒè¯æƒé‡æ˜¯å¦æ­£ç¡®åŠ è½½
                verify_winrate = evaluate_vs_random(current_policy, args.triangle_size, num_trials=10)
                print(f"éªŒè¯: vs Random = {verify_winrate*100:.0f}% (åº”è¯¥æ¥è¿‘ {winrate_random*100:.0f}%)")
                
                if verify_winrate < 0.80:
                    print("âš ï¸ è­¦å‘Š: æƒé‡å¯èƒ½æœªæ­£ç¡®åŠ è½½!")
                else:
                    print("âœ… æˆåŠŸåˆ‡æ¢åˆ°é˜¶æ®µ1!")
                
                phase = 1
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é˜¶æ®µ1ç›®æ ‡
            if phase == 1 and winrate_greedy >= 0.90 and not phase1_completed:
                phase1_completed = True
                print("\n" + "=" * 60)
                print(f"ğŸ‰ é˜¶æ®µ1å®Œæˆ! vs Greedyè¾¾åˆ° {winrate_greedy*100:.0f}%")
                print("ç°åœ¨åˆ‡æ¢åˆ°é˜¶æ®µ2: å¯¹æŠ—RL Baseline (ç›®æ ‡: 90%+)")
                print("=" * 60 + "\n")
                
                # 1. ä¿å­˜é˜¶æ®µ1å®Œæˆçš„checkpointï¼ˆä½¿ç”¨æœ€ä½³vs_greedyçš„checkpointï¼‰
                phase1_checkpoint = f"{logdir}/best_vs_greedy"
                if not os.path.exists(phase1_checkpoint):
                    print("è­¦å‘Š: æœ€ä½³checkpointä¸å­˜åœ¨ï¼Œä¿å­˜å½“å‰çŠ¶æ€...")
                    phase1_checkpoint = f"{logdir}/phase1_completed"
                    algo.save(checkpoint_dir=phase1_checkpoint)
                else:
                    print(f"ä½¿ç”¨æœ€ä½³checkpoint: {phase1_checkpoint}")
                
                # 2. éªŒè¯checkpointæ–‡ä»¶å­˜åœ¨
                policy_checkpoint_path = os.path.join(phase1_checkpoint, "policies", "default_policy")
                if not os.path.exists(policy_checkpoint_path):
                    print(f"é”™è¯¯: Checkpointè·¯å¾„ä¸å­˜åœ¨: {policy_checkpoint_path}")
                    print("ç»§ç»­è®­ç»ƒè€Œä¸åˆ‡æ¢é˜¶æ®µ...")
                    continue
                
                # 3. å…ˆåŠ è½½æƒé‡ï¼Œå†åˆ‡æ¢ç¯å¢ƒ
                print("æ­£åœ¨åŠ è½½é˜¶æ®µ1æƒé‡...")
                try:
                    phase1_policy = Policy.from_checkpoint(policy_checkpoint_path)
                    phase1_weights = phase1_policy.get_weights()
                    print(f"æˆåŠŸåŠ è½½æƒé‡ï¼Œå…± {len(phase1_weights)} ä¸ªå‚æ•°")
                except Exception as e:
                    print(f"é”™è¯¯: æ— æ³•åŠ è½½é˜¶æ®µ1æƒé‡: {e}")
                    print("ç»§ç»­è®­ç»ƒè€Œä¸åˆ‡æ¢é˜¶æ®µ...")
                    continue
                
                # 4. åœæ­¢å½“å‰ç®—æ³•
                algo.stop()
                
                # 5. é‡æ–°æ³¨å†Œç¯å¢ƒä¸ºRL Baseline
                print("é‡æ–°æ³¨å†Œç¯å¢ƒä¸ºå¯¹æŠ—RL Baseline...")
                register_env(env_name, env_creator_rl)
                config = create_config(env_name, args.triangle_size, args.num_workers, use_large_network=use_large)
                algo = config.build(logger_creator=custom_log_creator(os.path.join(os.curdir, logdir), ''))
                
                # 6. è®¾ç½®æƒé‡å¹¶åŒæ­¥
                print("å°†æƒé‡è®¾ç½®åˆ°æ–°ç®—æ³•...")
                try:
                    current_policy = algo.get_policy("default_policy")
                    current_policy.set_weights(phase1_weights)
                    
                    # åŒæ­¥åˆ°æ‰€æœ‰worker
                    algo.workers.local_worker().set_weights({"default_policy": phase1_weights})
                    def set_weights_fn(worker):
                        worker.set_weights({"default_policy": phase1_weights})
                    algo.workers.foreach_worker(set_weights_fn, local_worker=False)
                    
                    # åŒæ­¥åˆ°Learneræ¨¡å—ï¼ˆå…³é”®ï¼ï¼‰
                    learner_group = algo.learner_group
                    def update_learner_weights(learner):
                        learner._module["default_policy"].load_state_dict(
                            current_policy.model.state_dict()
                        )
                    learner_group.foreach_learner(update_learner_weights)
                    
                    print("âœ… æˆåŠŸåŠ è½½é˜¶æ®µ1æƒé‡å¹¶åŒæ­¥åˆ°æ‰€æœ‰workerå’ŒLearner!")
                    
                    # 7. éªŒè¯æƒé‡æ˜¯å¦æ­£ç¡®åŠ è½½ï¼ˆæµ‹è¯•vs Greedyï¼‰
                    verify_winrate = evaluate_vs_greedy(current_policy, args.triangle_size, num_trials=10)
                    print(f"éªŒè¯: vs Greedy = {verify_winrate*100:.0f}% (åº”è¯¥æ¥è¿‘ {winrate_greedy*100:.0f}%)")
                    if verify_winrate < 0.80:
                        print("âš ï¸ è­¦å‘Š: æƒé‡å¯èƒ½æœªæ­£ç¡®åŠ è½½ï¼Œèƒœç‡ä¸‹é™æ˜æ˜¾!")
                    
                except Exception as e:
                    print(f"é”™è¯¯: è®¾ç½®æƒé‡å¤±è´¥: {e}")
                    print("å°†ä»å¤´å¼€å§‹é˜¶æ®µ2...")
                
                # 8. åˆ‡æ¢é˜¶æ®µ
                phase = 2
                print("å¼€å§‹é˜¶æ®µ2è®­ç»ƒ...")
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€ç»ˆç›®æ ‡
            # æ£€æŸ¥æ˜¯å¦ä¸¤ä¸ªç›®æ ‡éƒ½è¾¾æˆ
            if winrate_greedy >= 0.90 and winrate_rl >= 0.90:
                print("\n" + "=" * 60)
                print(f"ğŸŠ è®­ç»ƒå®Œæˆ! vs Greedy={winrate_greedy*100:.0f}%, vs RL={winrate_rl*100:.0f}%")
                print("=" * 60)
                break
        
        # å®šæœŸä¿å­˜
        if i % 50 == 0:
            checkpoint_dir = f"{logdir}/checkpoint_{i}"
            algo.save(checkpoint_dir=checkpoint_dir)
    
    # æœ€ç»ˆè¯„ä¼°
    policy = algo.get_policy("default_policy")
    final_greedy = evaluate_vs_greedy(policy, args.triangle_size, num_trials=50)
    final_rl = evaluate_vs_rl_baseline(policy, rl_baseline, args.triangle_size, num_trials=50)
    print("="*60)
    print(f"è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ç»ˆ vs Greedy: {final_greedy*100:.1f}%")
    print(f"æœ€ç»ˆ vs RL Baseline: {final_rl*100:.1f}%")
    print(f"æœ€ä½³ vs RL: {best_winrate_rl*100:.1f}%")
    print(f"æ¨¡å‹ä¿å­˜åœ¨: {logdir}")
    
    ray.shutdown()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(prog='Train vs Greedy')
    parser.add_argument('--train_iters', type=int, default=200)
    parser.add_argument('--triangle_size', type=int, default=2)
    parser.add_argument('--num_cpus', type=int, default=16)
    parser.add_argument('--num_workers', type=int, default=8, help='å¹¶è¡Œé‡‡æ ·workeræ•°é‡')
    parser.add_argument('--eval_period', type=int, default=20, help='è¯„ä¼°é—´éš”')
    parser.add_argument('--restore_from', type=str, default=None, help='ä»checkpointæ¢å¤è®­ç»ƒ')
    parser.add_argument('--start_from_pretrained', action='store_true', help='ä»pretrained RL Baselineå¼€å§‹ï¼Œè·³è¿‡é˜¶æ®µ0')
    parser.add_argument('--local_mode', action='store_true')
    args = parser.parse_args()
    main(args)
