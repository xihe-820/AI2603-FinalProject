"""
å¯¹æŠ—è®­ç»ƒè„šæœ¬
è®­ç»ƒagentå¯¹æŠ—Greedyç­–ç•¥ï¼Œè¿™æ ·å¯ä»¥å­¦ä¹ å¦‚ä½•å‡»è´¥å¯¹æ‰‹
"""
import datetime
import os
import numpy as np
import argparse
from tqdm import tqdm

from gymnasium.spaces import Box, Discrete, Dict as GymDict
import gymnasium as gym

import ray
from ray.tune.registry import register_env
from ray.rllib.env.wrappers.pettingzoo_env import PettingZooEnv
from ray.rllib.algorithms.ppo import PPO, PPOConfig
from ray.rllib.core.rl_module.rl_module import SingleAgentRLModuleSpec
from ray.rllib.policy.policy import Policy

from ChineseChecker import chinese_checker_v0
from ChineseChecker.models.action_masking_rlm import TorchActionMaskRLM
from ChineseChecker.logger import custom_log_creator
from agents import GreedyPolicy

class SingleAgentVsOpponent(gym.Env):
    """å•agentç¯å¢ƒåŒ…è£…å™¨ï¼šagentå¯¹æŠ—å›ºå®šå¯¹æ‰‹ï¼ˆGreedyæˆ–RL Baselineï¼‰"""
    def __init__(self, triangle_size=2, max_iters=200, opponent_type='greedy', rl_opponent_policy=None):
        super().__init__()
        self.triangle_size = triangle_size
        self.max_iters = max_iters
        self.opponent_type = opponent_type
        
        # åˆå§‹åŒ–å¯¹æ‰‹
        if opponent_type == 'greedy':
            self.opponent = GreedyPolicy(triangle_size)
        elif opponent_type == 'rl_baseline':
            if rl_opponent_policy is None:
                self.opponent = Policy.from_checkpoint("pretrained/policies/default_policy")
            else:
                self.opponent = rl_opponent_policy
        else:
            raise ValueError(f"Unknown opponent type: {opponent_type}")
            
        self.env = chinese_checker_v0.env(
            render_mode=None, 
            triangle_size=triangle_size, 
            max_iters=max_iters
        )
        
        # å®šä¹‰è§‚æµ‹å’ŒåŠ¨ä½œç©ºé—´
        action_space_dim = (4 * triangle_size + 1) ** 2 * 6 * 2 + 1
        observation_space_dim = (4 * triangle_size + 1) ** 2 * 4
        
        # ä½¿ç”¨Dictç©ºé—´åŒ…å«observationå’Œaction_mask
        self.observation_space = GymDict({
            "observation": Box(low=0, high=1, shape=(observation_space_dim,), dtype=np.int8),
            "action_mask": Box(low=0, high=1, shape=(action_space_dim,), dtype=np.int8)
        })
        self.action_space = Discrete(action_space_dim)
        
    def reset(self, seed=None, options=None):
        """é‡ç½®ç¯å¢ƒ"""
        self.env.reset(seed=seed)
        
        # å¦‚æœç¬¬ä¸€ä¸ªç©å®¶æ˜¯å¯¹æ‰‹ï¼Œè®©å®ƒå…ˆèµ°
        if self.env.agent_selection == self.env.possible_agents[1]:
            obs, reward, termination, truncation, info = self.env.last()
            if not (termination or truncation):
                action = self.opponent.compute_single_action(obs)[0]
                self.env.step(int(action))
        
        # è¿”å›å­¦ä¹ agentçš„è§‚æµ‹
        obs, reward, termination, truncation, info = self.env.last()
        return obs, info
    
    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        # å­¦ä¹ agentèµ°ä¸€æ­¥
        self.env.step(int(action))
        obs, reward, terminated, truncated, info = self.env.last()
        
        done = terminated or truncated
        
        # å¦‚æœæ¸¸æˆç»“æŸï¼Œæ£€æŸ¥è°èµ¢äº†
        if done:
            # agentåˆšèµ°å®Œæ¸¸æˆå°±ç»“æŸï¼Œæ£€æŸ¥æ˜¯å¦agentèµ¢äº†
            winner = self.env.unwrapped.winner
            if winner == self.env.possible_agents[0]:
                # Agentæ˜¯player_0ï¼Œèµ¢äº†
                reward = 1000
            else:
                # Agentè¾“äº†
                reward = -1000
        # å¦‚æœæ¸¸æˆæ²¡ç»“æŸï¼Œè®©å¯¹æ‰‹èµ°
        elif self.env.agent_selection == self.env.possible_agents[1]:
            opp_obs = obs
            opp_action = self.opponent.compute_single_action(opp_obs)[0]
            self.env.step(int(opp_action))
            obs, opp_reward, terminated, truncated, info = self.env.last()
            
            done = terminated or truncated
            # å¦‚æœå¯¹æ‰‹èµ°å®Œåæ¸¸æˆç»“æŸï¼Œæ£€æŸ¥è°èµ¢äº†
            if done:
                winner = self.env.unwrapped.winner
                if winner == self.env.possible_agents[0]:
                    reward = 1000
                else:
                    reward = -1000
            else:
                reward = 0
        
        return obs, reward, done, done, info


def create_config(env_name: str, triangle_size: int = 4, num_workers: int = 8):
    """åˆ›å»ºPPOé…ç½®"""
    rlm_class = TorchActionMaskRLM
    model_config = {"fcnet_hiddens": [512, 512, 256]}  # æ›´å¤§çš„ç½‘ç»œ
    rlm_spec = SingleAgentRLModuleSpec(module_class=rlm_class, model_config_dict=model_config)

    action_space_dim = (4 * triangle_size + 1) ** 2 * 6 * 2 + 1
    observation_space_dim = (4 * triangle_size + 1) ** 2 * 4

    # è‡ªåŠ¨æ£€æµ‹GPU
    import torch
    num_gpus = 1 if torch.cuda.is_available() else 0
    if num_gpus > 0:
        print(f"æ£€æµ‹åˆ°GPU: {torch.cuda.get_device_name(0)}, å°†ä½¿ç”¨GPUè®­ç»ƒ")
    else:
        print("æœªæ£€æµ‹åˆ°GPU, ä½¿ç”¨CPUè®­ç»ƒ")
    print(f"ä½¿ç”¨ {num_workers} ä¸ªå¹¶è¡Œworkerè¿›è¡Œç¯å¢ƒé‡‡æ ·")

    config = (
        PPOConfig()
        .environment(
            env=env_name,
            clip_actions=True,
            env_config={
                "triangle_size": triangle_size,
                "max_iters": 200,
            },
        )
        .rollouts(
            num_rollout_workers=num_workers,  # å¢åŠ å¹¶è¡Œworker
            num_envs_per_worker=2,            # æ¯ä¸ªworkerè¿è¡Œ2ä¸ªç¯å¢ƒ
            rollout_fragment_length="auto",
        )
        .training(
            train_batch_size=4096,
            lr=5e-5,                          # é™ä½å­¦ä¹ ç‡ï¼Œæ›´ç¨³å®š
            gamma=0.995,
            lambda_=0.95,
            use_gae=True,
            clip_param=0.2,
            grad_clip=0.5,
            vf_loss_coeff=0.5,
            sgd_minibatch_size=512,
            num_sgd_iter=10,
            entropy_coeff=0.01,               # å¢åŠ ç†µï¼Œä¿ƒè¿›æ¢ç´¢
            _enable_learner_api=True
        )
        .experimental(_disable_preprocessor_api=True)
        .framework("torch")
        .resources(num_gpus=num_gpus)
        .rl_module(rl_module_spec=rlm_spec)
    )
    return config


def evaluate_vs_greedy(policy, triangle_size, num_trials=20):
    """è¯„ä¼°ç­–ç•¥å¯¹æŠ—Greedy"""
    env = chinese_checker_v0.env(render_mode=None, triangle_size=triangle_size, max_iters=100)
    greedy = GreedyPolicy(triangle_size)
    
    wins = 0
    for i in range(num_trials):
        env.reset(seed=i)
        for agent in env.agent_iter():
            obs, reward, termination, truncation, info = env.last()
            if termination or truncation:
                break
            
            # æ£€æŸ¥observationæ ¼å¼å¹¶å¤„ç†
            if agent == env.possible_agents[0]:
                # RLç­–ç•¥ï¼šå¯èƒ½éœ€è¦å¤„ç†dictæ ¼å¼
                try:
                    action = policy.compute_single_action(obs)[0]
                except Exception as e:
                    # å¦‚æœobsæ˜¯dictï¼Œå°è¯•å±•å¹³
                    if isinstance(obs, dict) and "observation" in obs:
                        action = policy.compute_single_action(obs["observation"])[0]
                    else:
                        raise e
            else:
                action = greedy.compute_single_action(obs)[0]
            env.step(int(action))
        
        if env.unwrapped.winner == env.possible_agents[0]:
            wins += 1
    
    return wins / num_trials


def evaluate_vs_rl_baseline(policy, rl_baseline, triangle_size, num_trials=20):
    """è¯„ä¼°ç­–ç•¥å¯¹æŠ—RL Baseline"""
    env = chinese_checker_v0.env(render_mode=None, triangle_size=triangle_size, max_iters=100)
    
    wins = 0
    for i in range(num_trials):
        env.reset(seed=i)
        for agent in env.agent_iter():
            obs, reward, termination, truncation, info = env.last()
            if termination or truncation:
                break
            
            # æ£€æŸ¥observationæ ¼å¼å¹¶å¤„ç†
            if agent == env.possible_agents[0]:
                try:
                    action = policy.compute_single_action(obs)[0]
                except Exception as e:
                    if isinstance(obs, dict) and "observation" in obs:
                        action = policy.compute_single_action(obs["observation"])[0]
                    else:
                        raise e
            else:
                try:
                    action = rl_baseline.compute_single_action(obs)[0]
                except Exception as e:
                    if isinstance(obs, dict) and "observation" in obs:
                        action = rl_baseline.compute_single_action(obs["observation"])[0]
                    else:
                        raise e
            env.step(int(action))
        
        if env.unwrapped.winner == env.possible_agents[0]:
            wins += 1
    
    return wins / num_trials


def train_vs_greedy_env(policy, greedy_policy, env, num_episodes=100):
    """
    è®©RLç­–ç•¥ä¸Greedyå¯¹å¼ˆæ”¶é›†ç»éªŒ
    è¿”å›transitionsç”¨äºè®­ç»ƒ
    """
    transitions = []
    
    for ep in range(num_episodes):
        env.reset(seed=ep)
        episode_data = []
        
        for agent in env.agent_iter():
            obs, reward, termination, truncation, info = env.last()
            if termination or truncation:
                break
            
            if agent == env.possible_agents[0]:
                # RL agent
                action = policy.compute_single_action(obs)[0]
                episode_data.append({
                    'obs': obs,
                    'action': action,
                    'reward': reward
                })
            else:
                # Greedy opponent
                action = greedy_policy.compute_single_action(obs)[0]
            
            env.step(int(action))
        
        # æ·»åŠ æœ€ç»ˆå¥–åŠ±
        if episode_data:
            final_reward = 1000 if env.unwrapped.winner == env.possible_agents[0] else -500
            episode_data[-1]['reward'] += final_reward
        
        transitions.extend(episode_data)
    
    return transitions


def main(args):
    """ä¸»å‡½æ•° - ä¸¤é˜¶æ®µè®­ç»ƒ"""
    
    # é˜¶æ®µ1ç¯å¢ƒï¼šå¯¹æŠ—Greedy
    def env_creator_greedy(config):
        return SingleAgentVsOpponent(
            triangle_size=config.get("triangle_size", 2),
            max_iters=config.get("max_iters", 200),
            opponent_type='greedy'
        )
    
    # é˜¶æ®µ2ç¯å¢ƒï¼šå¯¹æŠ—RL Baseline
    def env_creator_rl(config):
        return SingleAgentVsOpponent(
            triangle_size=config.get("triangle_size", 2),
            max_iters=config.get("max_iters", 200),
            opponent_type='rl_baseline'
        )

    env_name = 'single_vs_opponent'
    # å…ˆæ³¨å†ŒGreedyç¯å¢ƒ
    register_env(env_name, env_creator_greedy)

    ray.init(num_cpus=args.num_cpus or None, local_mode=args.local_mode)
    
    config = create_config(env_name, args.triangle_size, args.num_workers)
    
    timestr = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    logdir = f"logs/two_stage_{timestr}"
    os.makedirs(logdir, exist_ok=True)
    
    algo = config.build(logger_creator=custom_log_creator(os.path.join(os.curdir, logdir), ''))
    
    # ä»checkpointæ¢å¤æƒé‡
    if args.restore_from:
        print(f"ä»checkpointæ¢å¤æƒé‡: {args.restore_from}")
        try:
            # å°è¯•åŠ è½½policyçš„æƒé‡
            restored_policy = Policy.from_checkpoint(os.path.join(args.restore_from, "policies", "default_policy"))
            current_policy = algo.get_policy("default_policy")
            current_policy.set_weights(restored_policy.get_weights())
            # åŒæ­¥æƒé‡åˆ°æ‰€æœ‰worker
            algo.workers.sync_weights()
            print("æˆåŠŸæ¢å¤æƒé‡å¹¶åŒæ­¥åˆ°æ‰€æœ‰worker!")
        except Exception as e:
            print(f"æ— æ³•ä»checkpointæ¢å¤æƒé‡: {e}")
            print("å°†ä»å¤´å¼€å§‹è®­ç»ƒ...")
    
    greedy = GreedyPolicy(args.triangle_size)
    
    # åŠ è½½RL Baselineç”¨äºè¯„ä¼°
    rl_baseline = Policy.from_checkpoint("pretrained/policies/default_policy")
    
    best_winrate_greedy = 0.0
    best_winrate_rl = 0.0
    phase = 1  # 1=å¯¹æŠ—Greedy, 2=å¯¹æŠ—RL Baseline
    phase1_completed = False
    
    print("=" * 60)
    print("é˜¶æ®µ1: å¯¹æŠ—Greedyè®­ç»ƒ (ç›®æ ‡: 90%+)")
    print("=" * 60)
    
    for i in range(args.train_iters):
        # è®­ç»ƒä¸€æ¬¡è¿­ä»£
        result = algo.train()
        
        # è·å–ç­–ç•¥
        policy = algo.get_policy("default_policy")
        
        # æ¯Næ¬¡è¯„ä¼°ä¸€ä¸‹
        if i % args.eval_period == 0:
            winrate_greedy = evaluate_vs_greedy(policy, args.triangle_size, num_trials=10)
            winrate_rl = evaluate_vs_rl_baseline(policy, rl_baseline, args.triangle_size, num_trials=10)
            
            print(f"[é˜¶æ®µ{phase}] Iter {i}: reward={result['episode_reward_mean']:.1f}, "
                  f"vs_Greedy={winrate_greedy*100:.0f}%, vs_RL={winrate_rl*100:.0f}%")
            
            # ä¿å­˜vs Greedyæœ€å¥½çš„æ¨¡å‹
            if winrate_greedy > best_winrate_greedy:
                best_winrate_greedy = winrate_greedy
                checkpoint_dir = f"{logdir}/best_vs_greedy"
                algo.save(checkpoint_dir=checkpoint_dir)
                print(f"  -> æ–°æœ€ä½³vs Greedy: {winrate_greedy*100:.0f}%")
            
            # ä¿å­˜vs RLæœ€å¥½çš„æ¨¡å‹
            if winrate_rl > best_winrate_rl:
                best_winrate_rl = winrate_rl
                checkpoint_dir = f"{logdir}/best_vs_rl"
                algo.save(checkpoint_dir=checkpoint_dir)
                print(f"  -> æ–°æœ€ä½³vs RL: {winrate_rl*100:.0f}%")
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é˜¶æ®µ1ç›®æ ‡
            if phase == 1 and winrate_greedy >= 0.90 and not phase1_completed:
                phase1_completed = True
                print("\n" + "=" * 60)
                print(f"ğŸ‰ é˜¶æ®µ1å®Œæˆ! vs Greedyè¾¾åˆ° {winrate_greedy*100:.0f}%")
                print("ç°åœ¨åˆ‡æ¢åˆ°é˜¶æ®µ2: å¯¹æŠ—RL Baseline (ç›®æ ‡: 90%+)")
                print("=" * 60 + "\n")
                
                # 1. ä¿å­˜é˜¶æ®µ1å®Œæˆçš„checkpointï¼ˆä½¿ç”¨æœ€ä½³vs_greedyçš„checkpointï¼‰
                phase1_checkpoint = f"{logdir}/best_vs_greedy"
                if not os.path.exists(phase1_checkpoint):
                    print("è­¦å‘Š: æœ€ä½³checkpointä¸å­˜åœ¨ï¼Œä¿å­˜å½“å‰çŠ¶æ€...")
                    phase1_checkpoint = f"{logdir}/phase1_completed"
                    algo.save(checkpoint_dir=phase1_checkpoint)
                else:
                    print(f"ä½¿ç”¨æœ€ä½³checkpoint: {phase1_checkpoint}")
                
                # 2. éªŒè¯checkpointæ–‡ä»¶å­˜åœ¨
                policy_checkpoint_path = os.path.join(phase1_checkpoint, "policies", "default_policy")
                if not os.path.exists(policy_checkpoint_path):
                    print(f"é”™è¯¯: Checkpointè·¯å¾„ä¸å­˜åœ¨: {policy_checkpoint_path}")
                    print("ç»§ç»­è®­ç»ƒè€Œä¸åˆ‡æ¢é˜¶æ®µ...")
                    continue
                
                # 3. å…ˆåŠ è½½æƒé‡ï¼Œå†åˆ‡æ¢ç¯å¢ƒ
                print("æ­£åœ¨åŠ è½½é˜¶æ®µ1æƒé‡...")
                try:
                    phase1_policy = Policy.from_checkpoint(policy_checkpoint_path)
                    phase1_weights = phase1_policy.get_weights()
                    print(f"æˆåŠŸåŠ è½½æƒé‡ï¼Œå…± {len(phase1_weights)} ä¸ªå‚æ•°")
                except Exception as e:
                    print(f"é”™è¯¯: æ— æ³•åŠ è½½é˜¶æ®µ1æƒé‡: {e}")
                    print("ç»§ç»­è®­ç»ƒè€Œä¸åˆ‡æ¢é˜¶æ®µ...")
                    continue
                
                # 4. åœæ­¢å½“å‰ç®—æ³•
                algo.stop()
                
                # 5. é‡æ–°æ³¨å†Œç¯å¢ƒä¸ºRL Baseline
                print("é‡æ–°æ³¨å†Œç¯å¢ƒä¸ºå¯¹æŠ—RL Baseline...")
                register_env(env_name, env_creator_rl)
                config = create_config(env_name, args.triangle_size, args.num_workers)
                algo = config.build(logger_creator=custom_log_creator(os.path.join(os.curdir, logdir), ''))
                
                # 6. è®¾ç½®æƒé‡å¹¶åŒæ­¥
                print("å°†æƒé‡è®¾ç½®åˆ°æ–°ç®—æ³•...")
                try:
                    current_policy = algo.get_policy("default_policy")
                    current_policy.set_weights(phase1_weights)
                    algo.workers.sync_weights()
                    print("âœ… æˆåŠŸåŠ è½½é˜¶æ®µ1æƒé‡å¹¶åŒæ­¥åˆ°æ‰€æœ‰worker!")
                    
                    # 7. éªŒè¯æƒé‡æ˜¯å¦æ­£ç¡®åŠ è½½ï¼ˆæµ‹è¯•vs Greedyï¼‰
                    verify_winrate = evaluate_vs_greedy(current_policy, args.triangle_size, num_trials=10)
                    print(f"éªŒè¯: vs Greedy = {verify_winrate*100:.0f}% (åº”è¯¥æ¥è¿‘ {winrate_greedy*100:.0f}%)")
                    if verify_winrate < 0.80:
                        print("âš ï¸ è­¦å‘Š: æƒé‡å¯èƒ½æœªæ­£ç¡®åŠ è½½ï¼Œèƒœç‡ä¸‹é™æ˜æ˜¾!")
                    
                except Exception as e:
                    print(f"é”™è¯¯: è®¾ç½®æƒé‡å¤±è´¥: {e}")
                    print("å°†ä»å¤´å¼€å§‹é˜¶æ®µ2...")
                
                # 8. åˆ‡æ¢é˜¶æ®µ
                phase = 2
                print("å¼€å§‹é˜¶æ®µ2è®­ç»ƒ...")
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€ç»ˆç›®æ ‡
            # æ£€æŸ¥æ˜¯å¦ä¸¤ä¸ªç›®æ ‡éƒ½è¾¾æˆ
            if winrate_greedy >= 0.90 and winrate_rl >= 0.90:
                print("\n" + "=" * 60)
                print(f"ğŸŠ è®­ç»ƒå®Œæˆ! vs Greedy={winrate_greedy*100:.0f}%, vs RL={winrate_rl*100:.0f}%")
                print("=" * 60)
                break
        
        # å®šæœŸä¿å­˜
        if i % 50 == 0:
            checkpoint_dir = f"{logdir}/checkpoint_{i}"
            algo.save(checkpoint_dir=checkpoint_dir)
    
    # æœ€ç»ˆè¯„ä¼°
    policy = algo.get_policy("default_policy")
    final_greedy = evaluate_vs_greedy(policy, args.triangle_size, num_trials=50)
    final_rl = evaluate_vs_rl_baseline(policy, rl_baseline, args.triangle_size, num_trials=50)
    print("="*60)
    print(f"è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ç»ˆ vs Greedy: {final_greedy*100:.1f}%")
    print(f"æœ€ç»ˆ vs RL Baseline: {final_rl*100:.1f}%")
    print(f"æœ€ä½³ vs RL: {best_rl_winrate*100:.1f}%")
    print(f"æ¨¡å‹ä¿å­˜åœ¨: {logdir}")
    
    ray.shutdown()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(prog='Train vs Greedy')
    parser.add_argument('--train_iters', type=int, default=200)
    parser.add_argument('--triangle_size', type=int, default=2)
    parser.add_argument('--num_cpus', type=int, default=16)
    parser.add_argument('--num_workers', type=int, default=8, help='å¹¶è¡Œé‡‡æ ·workeræ•°é‡')
    parser.add_argument('--eval_period', type=int, default=20, help='è¯„ä¼°é—´éš”')
    parser.add_argument('--restore_from', type=str, default=None, help='ä»checkpointæ¢å¤è®­ç»ƒ')
    parser.add_argument('--local_mode', action='store_true')
    args = parser.parse_args()
    main(args)
