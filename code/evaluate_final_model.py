"""
è¯„ä¼°æœ€ç»ˆè®­ç»ƒæ¨¡å‹çš„æ€§èƒ½
å¯¹æŠ—Greedyå’ŒRL Baselineè¿›è¡Œå¤šæ¬¡å¯¹å±€
"""
import os
import argparse
from tqdm import tqdm

from ray.rllib.policy.policy import Policy

from ChineseChecker import chinese_checker_v0
from agents import GreedyPolicy


def evaluate_vs_greedy(policy, triangle_size, num_trials=100):
    """è¯„ä¼°ç­–ç•¥å¯¹æŠ—Greedy"""
    env = chinese_checker_v0.env(render_mode=None, triangle_size=triangle_size, max_iters=100)
    greedy = GreedyPolicy(triangle_size)
    
    wins = 0
    for i in tqdm(range(num_trials), desc="vs Greedy"):
        env.reset(seed=i)
        for agent in env.agent_iter():
            obs, reward, termination, truncation, info = env.last()
            if termination or truncation:
                break
            
            if agent == env.possible_agents[0]:
                action = policy.compute_single_action(obs)[0]
            else:
                action = greedy.compute_single_action(obs)[0]
            env.step(int(action))
        
        if env.unwrapped.winner == env.possible_agents[0]:
            wins += 1
    
    return wins / num_trials


def evaluate_vs_rl_baseline(policy, rl_baseline, triangle_size, num_trials=100):
    """è¯„ä¼°ç­–ç•¥å¯¹æŠ—RL Baseline"""
    env = chinese_checker_v0.env(render_mode=None, triangle_size=triangle_size, max_iters=100)
    
    wins = 0
    for i in tqdm(range(num_trials), desc="vs RL Baseline"):
        env.reset(seed=i)
        for agent in env.agent_iter():
            obs, reward, termination, truncation, info = env.last()
            if termination or truncation:
                break
            
            if agent == env.possible_agents[0]:
                action = policy.compute_single_action(obs)[0]
            else:
                action = rl_baseline.compute_single_action(obs)[0]
            env.step(int(action))
        
        if env.unwrapped.winner == env.possible_agents[0]:
            wins += 1
    
    return wins / num_trials


def evaluate_vs_random(policy, triangle_size, num_trials=100):
    """è¯„ä¼°ç­–ç•¥å¯¹æŠ—Random"""
    env = chinese_checker_v0.env(render_mode=None, triangle_size=triangle_size, max_iters=100)
    import random
    
    wins = 0
    for i in tqdm(range(num_trials), desc="vs Random"):
        env.reset(seed=i)
        random.seed(i)
        for agent in env.agent_iter():
            obs, reward, termination, truncation, info = env.last()
            if termination or truncation:
                break
            
            if agent == env.possible_agents[0]:
                action = policy.compute_single_action(obs)[0]
            else:
                # Randomé€‰æ‹©åˆæ³•åŠ¨ä½œ
                action_mask = obs["action_mask"]
                valid_actions = [j for j in range(len(action_mask)) if action_mask[j] == 1]
                action = random.choice(valid_actions) if valid_actions else 0
            env.step(int(action))
        
        if env.unwrapped.winner == env.possible_agents[0]:
            wins += 1
    
    return wins / num_trials


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--checkpoint", type=str, 
                       default="logs/three_stage_2025-12-31_19-49-51/best_vs_rl",
                       help="è®­ç»ƒå¥½çš„æ¨¡å‹checkpointè·¯å¾„")
    parser.add_argument("--triangle_size", type=int, default=2)
    parser.add_argument("--num_trials", type=int, default=100, help="è¯„ä¼°å¯¹å±€æ•°")
    args = parser.parse_args()
    
    print("=" * 60)
    print(f"è¯„ä¼°æœ€ç»ˆè®­ç»ƒæ¨¡å‹: {args.checkpoint}")
    print(f"å¯¹å±€æ•°: {args.num_trials}")
    print("=" * 60)
    
    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    policy_path = os.path.join(args.checkpoint, "policies", "default_policy")
    if not os.path.exists(policy_path):
        policy_path = args.checkpoint  # å¯èƒ½ç›´æ¥æ˜¯policyè·¯å¾„
    
    print(f"\nåŠ è½½æ¨¡å‹: {policy_path}")
    trained_policy = Policy.from_checkpoint(policy_path)
    
    # åŠ è½½RL Baseline
    print("åŠ è½½RL Baseline: pretrained/policies/default_policy")
    rl_baseline = Policy.from_checkpoint("pretrained/policies/default_policy")
    
    # è¯„ä¼°
    print("\n" + "=" * 60)
    print("å¼€å§‹è¯„ä¼°...")
    print("=" * 60)
    
    # vs Random
    winrate_random = evaluate_vs_random(trained_policy, args.triangle_size, args.num_trials)
    print(f"\nâœ“ vs Random: {winrate_random*100:.1f}%")
    
    # vs Greedy
    winrate_greedy = evaluate_vs_greedy(trained_policy, args.triangle_size, args.num_trials)
    print(f"âœ“ vs Greedy: {winrate_greedy*100:.1f}%")
    
    # vs RL Baseline
    winrate_rl = evaluate_vs_rl_baseline(trained_policy, rl_baseline, args.triangle_size, args.num_trials)
    print(f"âœ“ vs RL Baseline: {winrate_rl*100:.1f}%")
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æœ")
    print("=" * 60)
    print(f"  vs Random:      {winrate_random*100:.1f}%")
    print(f"  vs Greedy:      {winrate_greedy*100:.1f}%")
    print(f"  vs RL Baseline: {winrate_rl*100:.1f}%")
    print("=" * 60)
    
    # åˆ¤æ–­æ˜¯å¦è¾¾æ ‡
    if winrate_greedy >= 0.90 and winrate_rl >= 0.90:
        print("ğŸŠ æ­å–œï¼æ¨¡å‹è¾¾åˆ°ç›®æ ‡ (90%+ vs Greedy & RL Baseline)")
    elif winrate_greedy >= 0.90:
        print(f"âœ“ vs Greedyè¾¾æ ‡ï¼Œvs RLè¿˜éœ€æå‡ {(0.90 - winrate_rl)*100:.1f}%")
    else:
        print(f"éœ€è¦ç»§ç»­è®­ç»ƒ...")


if __name__ == "__main__":
    main()
